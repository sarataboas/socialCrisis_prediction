{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a6277b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def read_json(file_path): \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82027e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_base = read_json('../outputs/results_base.json')\n",
    "results_oversampling = read_json('../outputs/results_oversampling.json')\n",
    "results_focal_loss = read_json('../outputs/results_focal_loss.json')\n",
    "results_class_weights = read_json('../outputs/results_class_weights.json')\n",
    "results_cost_sensitive = read_json('../outputs/results_cost_sensitive.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7317c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def extract_metrics(results_dict, positive_label=\"1\"):\n",
    "    cm = results_dict[\"confusion_matrix\"]\n",
    "    tn, fp = cm[0]\n",
    "    fn, tp = cm[1]\n",
    "\n",
    "    # Recall for each class\n",
    "    recall_pos = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    recall_neg = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "\n",
    "    gmean = np.sqrt(recall_pos * recall_neg)\n",
    "\n",
    "    return {\n",
    "        \"Accuracy\": results_dict[\"accuracy\"],\n",
    "        \"Balanced Accuracy\": results_dict[\"balanced_accuracy\"],\n",
    "        \"Precision (Crisis)\": results_dict[\"classification_report\"][positive_label][\"precision\"],\n",
    "        \"Recall (Crisis)\": recall_pos,\n",
    "        \"F1-score (Crisis)\": results_dict[\"classification_report\"][positive_label][\"f1-score\"],\n",
    "        \"G-Mean\": gmean\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a0e5ea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "      <th>Precision (Crisis)</th>\n",
       "      <th>Recall (Crisis)</th>\n",
       "      <th>F1-score (Crisis)</th>\n",
       "      <th>G-Mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Baseline LSTM</th>\n",
       "      <td>0.200</td>\n",
       "      <td>0.565</td>\n",
       "      <td>0.091</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Window Oversampling</th>\n",
       "      <td>0.640</td>\n",
       "      <td>0.804</td>\n",
       "      <td>0.182</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Focal Loss</th>\n",
       "      <td>0.520</td>\n",
       "      <td>0.739</td>\n",
       "      <td>0.143</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class Weights</th>\n",
       "      <td>0.762</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.167</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Adaptive Cost-Sensitive</th>\n",
       "      <td>0.714</td>\n",
       "      <td>0.850</td>\n",
       "      <td>0.143</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.837</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Accuracy  Balanced Accuracy  Precision (Crisis)  \\\n",
       "Baseline LSTM               0.200              0.565               0.091   \n",
       "Window Oversampling         0.640              0.804               0.182   \n",
       "Focal Loss                  0.520              0.739               0.143   \n",
       "Class Weights               0.762              0.875               0.167   \n",
       "Adaptive Cost-Sensitive     0.714              0.850               0.143   \n",
       "\n",
       "                         Recall (Crisis)  F1-score (Crisis)  G-Mean  \n",
       "Baseline LSTM                        1.0              0.167   0.361  \n",
       "Window Oversampling                  1.0              0.308   0.780  \n",
       "Focal Loss                           1.0              0.250   0.692  \n",
       "Class Weights                        1.0              0.286   0.866  \n",
       "Adaptive Cost-Sensitive              1.0              0.250   0.837  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_table = pd.DataFrame.from_dict(\n",
    "    {\n",
    "        \"Baseline LSTM\": extract_metrics(results_base),\n",
    "        \"Window Oversampling\": extract_metrics(results_oversampling),\n",
    "        \"Focal Loss\": extract_metrics(results_focal_loss),\n",
    "        \"Class Weights\": extract_metrics(results_class_weights),\n",
    "        \"Adaptive Cost-Sensitive\": extract_metrics(results_cost_sensitive),\n",
    "    },\n",
    "    orient=\"index\"\n",
    ")\n",
    "\n",
    "# Optional: nicer formatting\n",
    "results_table = results_table.round(3)\n",
    "\n",
    "results_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c273d6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}\n",
      "\\caption{Performance comparison of imbalance-handling techniques}\n",
      "\\label{tab:imbalance_results}\n",
      "\\begin{tabular}{lrrrrrr}\n",
      "\\toprule\n",
      " & Accuracy & Balanced Accuracy & Precision (Crisis) & Recall (Crisis) & F1-score (Crisis) & G-Mean \\\\\n",
      "\\midrule\n",
      "Baseline LSTM & 0.200 & 0.565 & 0.091 & 1.000 & 0.167 & 0.361 \\\\\n",
      "Window Oversampling & 0.640 & 0.804 & 0.182 & 1.000 & 0.308 & 0.780 \\\\\n",
      "Focal Loss & 0.520 & 0.739 & 0.143 & 1.000 & 0.250 & 0.692 \\\\\n",
      "Class Weights & 0.762 & 0.875 & 0.167 & 1.000 & 0.286 & 0.866 \\\\\n",
      "Adaptive Cost-Sensitive & 0.714 & 0.850 & 0.143 & 1.000 & 0.250 & 0.837 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    results_table.to_latex(\n",
    "        float_format=\"%.3f\",\n",
    "        caption=\"Performance comparison of imbalance-handling techniques\",\n",
    "        label=\"tab:imbalance_results\"\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
